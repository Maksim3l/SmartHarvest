{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a695e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abdb8b74",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011cff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d9d5f",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84576dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitInstanceDataset(Dataset):\n",
    "    def __init__(self, json_file, images_dir, transform=None, target_size=(1200, )):\n",
    "\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        self.class_map = {\n",
    "            'apple': {'ripe': 1, 'unripe': 2, 'spoiled': 3},\n",
    "            'cherry': {'ripe': 4, 'unripe': 5, 'spoiled': 6},\n",
    "            'cucumber': {'ripe': 7, 'unripe': 8, 'spoiled': 9},\n",
    "            'lettuce': {'ripe': 10, 'unripe': 11, 'spoiled': 12},\n",
    "            'pear': {'ripe': 13, 'unripe': 14, 'spoiled': 15},\n",
    "            'pepper': {'ripe': 16, 'unripe': 17, 'spoiled': 18},\n",
    "            'raspberry': {'ripe': 19, 'unripe': 20, 'spoiled': 21},\n",
    "            'strawberry': {'ripe': 22, 'unripe': 23, 'spoiled': 24},\n",
    "            'tomato': {'ripe': 25, 'unripe': 26, 'spoiled': 27},\n",
    "        }\n",
    "\n",
    "        self.class_names = ['background']\n",
    "\n",
    "        for plant, ripeness_dict in self.class_map.items():\n",
    "            for ripeness, class_id in ripeness_dict.items():\n",
    "                self.class_names.append(f\"{plant}-{ripeness}\")\n",
    "        self.valid_samples = []\n",
    "\n",
    "        for key, image_data in self.annotations.items():\n",
    "            if (isinstance(image_data, dict) and \n",
    "                'filename' in image_data and \n",
    "                'regions' in image_data and \n",
    "                len(image_data['regions']) > 0):\n",
    "                self.valid_samples.append(image_data)\n",
    "\n",
    "        print(f\"Loaded {len(self.valid_samples)} images with annotations\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_data = self.valid_samples[idx]\n",
    "        filename = image_data['filename']\n",
    "        plant_type = image_data.get('file_attributes', {}).get('plant', 'unknown').lower()\n",
    "        regions = image_data['regions']\n",
    "        image_path = self.find_image_file(filename)\n",
    "\n",
    "        if not image_path:\n",
    "            raise ValueError(f\"Image not found: {filename}\")\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        areas = []\n",
    "\n",
    "        for region in regions:\n",
    "\n",
    "            if not region or region.get('shape_attributes', {}).get('name') != 'polygon':\n",
    "                continue\n",
    "\n",
    "            x_points = region['shape_attributes'].get('all_points_x', [])\n",
    "            y_points = region['shape_attributes'].get('all_points_y', [])\n",
    "\n",
    "            if len(x_points) < 3 or len(y_points) < 3:\n",
    "                continue\n",
    "\n",
    "            polygon_coords = [(x, y) for x, y in zip(x_points, y_points)]\n",
    "            mask = np.zeros((height, width), dtype=np.uint8)\n",
    "            cv2.fillPoly(mask, [np.array(polygon_coords, dtype=np.int32)], 1)\n",
    "            area = np.sum(mask)\n",
    "\n",
    "            if area < 100:\n",
    "                continue\n",
    "\n",
    "            pos = np.where(mask)\n",
    "\n",
    "            if len(pos[0]) == 0:\n",
    "                continue\n",
    "\n",
    "            xmin, xmax = np.min(pos[1]), np.max(pos[1])\n",
    "            ymin, ymax = np.min(pos[0]), np.max(pos[0])\n",
    "\n",
    "            if xmax <= xmin or ymax <= ymin:\n",
    "                continue\n",
    "\n",
    "            ripeness = region.get('region_attributes', {}).get('ripeness_factor', 'ripe')\n",
    "            \n",
    "            if plant_type in self.class_map and ripeness in self.class_map[plant_type]:\n",
    "                label = self.class_map[plant_type][ripeness]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(label)\n",
    "            masks.append(mask)\n",
    "            areas.append(area)\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "            areas = torch.zeros((0,), dtype=torch.float32)\n",
    "\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "            areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'masks': masks,\n",
    "            'area': areas,\n",
    "            'iscrowd': torch.zeros((len(labels),), dtype=torch.int64),\n",
    "            'image_id': torch.tensor([idx])\n",
    "        }\n",
    "        if self.transform:\n",
    "            image = Image.fromarray(image)\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(Image.fromarray(image))\n",
    "        return image, target\n",
    "    \n",
    "    def find_image_file(self, filename):\n",
    "        direct_path = os.path.join(self.images_dir, filename)\n",
    "        if os.path.exists(direct_path):\n",
    "            return direct_path\n",
    "        for root, _, files in os.walk(self.images_dir):\n",
    "            if filename in files:\n",
    "                return os.path.join(root, filename)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5f88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_model(num_classes):\n",
    "\n",
    "    model = maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask, hidden_layer, num_classes\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee4bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde8807",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030635dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_instance_model():\n",
    "\n",
    "    config = {\n",
    "        'batch_size': 4,\n",
    "        'learning_rate': 5e-4,\n",
    "        'num_epochs': 50,\n",
    "        'num_classes': 28,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'save_dir': 'instance_checkpoints',\n",
    "        'warmup_epochs': 5,\n",
    "        'print_freq': 10\n",
    "    }\n",
    "\n",
    "    print(f\"Training on device: {config['device']}\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((1200, 1200)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    dataset = FruitInstanceDataset(\n",
    "        json_file=\"via_export_json(4).json\",\n",
    "        images_dir=\"Pictures/\",\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    print(f\"Training set: {len(train_dataset)} images\")\n",
    "    print(f\"Validation set: {len(val_dataset)} images\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model = get_instance_model(config['num_classes'])\n",
    "    model.to(config['device'])\n",
    "\n",
    "    params = [\n",
    "        {'params': [p for n, p in model.named_parameters() if 'backbone' in n], 'lr': config['learning_rate'] * 0.1},\n",
    "        {'params': [p for n, p in model.named_parameters() if 'backbone' not in n], 'lr': config['learning_rate']}\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[20, 35], gamma=0.1\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    os.makedirs(config['save_dir'], exist_ok=True)\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            images = [img.to(config['device']) for img in images]\n",
    "            targets = [{k: v.to(config['device']) for k, v in t.items()} \n",
    "                      for t in targets]\n",
    "            \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += losses.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            if batch_idx % config['print_freq'] == 0:\n",
    "                print(f\"Batch {batch_idx}/{len(train_loader)}, Loss: {losses.item():.4f}\")\n",
    "                for k, v in loss_dict.items():\n",
    "                    print(f\"  {k}: {v.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = [img.to(config['device']) for img in images]\n",
    "                targets = [{k: v.to(config['device']) for k, v in t.items()} \n",
    "                          for t in targets]\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += losses.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == config['num_epochs'] - 1:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'config': config,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses\n",
    "            }\n",
    "            checkpoint_path = os.path.join(config['save_dir'], f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "    final_model_path = os.path.join(config['save_dir'], \"final_model.pth\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config['save_dir'], 'training_curves.png'))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Final model saved: {final_model_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81f61f",
   "metadata": {},
   "source": [
    "# Pseudo Label - predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e3ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pseudo_labels_with_maskrcnn(model_path, unlabeled_images_dir, output_dir, \n",
    "                                       confidence_threshold=0.7, nms_threshold=0.5):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = get_instance_model(28)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    class_map = {\n",
    "        1: {'plant': 'apple', 'ripeness': 'ripe'},\n",
    "        2: {'plant': 'apple', 'ripeness': 'unripe'},\n",
    "        3: {'plant': 'apple', 'ripeness': 'spoiled'},\n",
    "        4: {'plant': 'cherry', 'ripeness': 'ripe'},\n",
    "        5: {'plant': 'cherry', 'ripeness': 'unripe'},\n",
    "        6: {'plant': 'cherry', 'ripeness': 'spoiled'},\n",
    "        7: {'plant': 'cucumber', 'ripeness': 'ripe'},\n",
    "        8: {'plant': 'cucumber', 'ripeness': 'unripe'},\n",
    "        9: {'plant': 'cucumber', 'ripeness': 'spoiled'},\n",
    "        10: {'plant': 'lettuce', 'ripeness': 'ripe'},\n",
    "        11: {'plant': 'lettuce', 'ripeness': 'unripe'},\n",
    "        12: {'plant': 'lettuce', 'ripeness': 'spoiled'},\n",
    "        13: {'plant': 'pear', 'ripeness': 'ripe'},\n",
    "        14: {'plant': 'pear', 'ripeness': 'unripe'},\n",
    "        15: {'plant': 'pear', 'ripeness': 'spoiled'},\n",
    "        16: {'plant': 'pepper', 'ripeness': 'ripe'},\n",
    "        17: {'plant': 'pepper', 'ripeness': 'unripe'},\n",
    "        18: {'plant': 'pepper', 'ripeness': 'spoiled'},\n",
    "        19: {'plant': 'raspberry', 'ripeness': 'ripe'},\n",
    "        20: {'plant': 'raspberry', 'ripeness': 'unripe'},\n",
    "        21: {'plant': 'raspberry', 'ripeness': 'spoiled'},\n",
    "        22: {'plant': 'strawberry', 'ripeness': 'ripe'},\n",
    "        23: {'plant': 'strawberry', 'ripeness': 'unripe'},\n",
    "        24: {'plant': 'strawberry', 'ripeness': 'spoiled'},\n",
    "        25: {'plant': 'tomato', 'ripeness': 'ripe'},\n",
    "        26: {'plant': 'tomato', 'ripeness': 'unripe'},\n",
    "        27: {'plant': 'tomato', 'ripeness': 'spoiled'},\n",
    "    }\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((1200, 1200)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pseudo_masks_dir = os.path.join(output_dir, 'pseudo_masks')\n",
    "    pseudo_annotations_dir = os.path.join(output_dir, 'pseudo_annotations')\n",
    "\n",
    "    os.makedirs(pseudo_masks_dir, exist_ok=True)\n",
    "    os.makedirs(pseudo_annotations_dir, exist_ok=True)\n",
    "\n",
    "    processed_count = 0\n",
    "    total_instances = 0\n",
    "\n",
    "    print(\"Generating pseudo labels...\")\n",
    "\n",
    "    for root, _, files in os.walk(unlabeled_images_dir):\n",
    "        for filename in files:\n",
    "            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                continue\n",
    "\n",
    "            image_path = os.path.join(root, filename)\n",
    "\n",
    "            try:\n",
    "\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                original_size = image.size\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "                    predictions = model(image_tensor)\n",
    "\n",
    "                pred = predictions[0]\n",
    "\n",
    "                keep_idx = pred['scores'] > confidence_threshold\n",
    "\n",
    "                if torch.sum(keep_idx) == 0:\n",
    "                    continue\n",
    "\n",
    "                keep_idx = torchvision.ops.nms(\n",
    "                    pred['boxes'][keep_idx], \n",
    "                    pred['scores'][keep_idx], \n",
    "                    nms_threshold\n",
    "                )\n",
    "\n",
    "                boxes = pred['boxes'][keep_idx]\n",
    "                labels = pred['labels'][keep_idx]\n",
    "                masks = pred['masks'][keep_idx]\n",
    "                scores = pred['scores'][keep_idx]\n",
    "\n",
    "                if len(boxes) == 0:\n",
    "                    continue\n",
    "\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                regions = []\n",
    "\n",
    "                height, width = original_size[1], original_size[0]\n",
    "                instance_mask = np.zeros((height, width), dtype=np.uint16)\n",
    "                semantic_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "                for i, (box, label, mask, score) in enumerate(zip(boxes, labels, masks, scores)):\n",
    "\n",
    "                    mask_np = mask[0].cpu().numpy()\n",
    "                    mask_resized = cv2.resize(mask_np, (width, height), \n",
    "                                            interpolation=cv2.INTER_LINEAR)\n",
    "                    mask_binary = mask_resized > 0.5\n",
    "\n",
    "                    if np.sum(mask_binary) < 100:\n",
    "                        continue\n",
    "\n",
    "                    contours, _ = cv2.findContours(\n",
    "                        mask_binary.astype(np.uint8), \n",
    "                        cv2.RETR_EXTERNAL, \n",
    "                        cv2.CHAIN_APPROX_SIMPLE\n",
    "                    )\n",
    "\n",
    "                    if len(contours) == 0:\n",
    "                        continue\n",
    "\n",
    "                    largest_contour = max(contours, key=cv2.contourArea)\n",
    "                    epsilon = 0.01 * cv2.arcLength(largest_contour, True)\n",
    "                    simplified = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
    "\n",
    "                    if len(simplified) < 3:\n",
    "                        continue\n",
    "\n",
    "                    x_points = [int(point[0][0]) for point in simplified]\n",
    "                    y_points = [int(point[0][1]) for point in simplified]\n",
    "\n",
    "                    class_id = label.item()\n",
    "                    \n",
    "                    if class_id in class_map:\n",
    "                        plant_type = class_map[class_id]['plant']\n",
    "                        ripeness = class_map[class_id]['ripeness']\n",
    "\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    region = {\n",
    "                        'shape_attributes': {\n",
    "                            'name': 'polygon',\n",
    "                            'all_points_x': x_points,\n",
    "                            'all_points_y': y_points\n",
    "                        },\n",
    "                        'region_attributes': {\n",
    "                            'ripeness_factor': ripeness\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    regions.append(region)\n",
    "                    instance_mask[mask_binary] = i + 1\n",
    "                    semantic_mask[mask_binary] = class_id\n",
    "                    total_instances += 1\n",
    "\n",
    "                if len(regions) == 0:\n",
    "                    continue\n",
    "\n",
    "                pseudo_annotation = {\n",
    "                    filename: {\n",
    "                        'filename': filename,\n",
    "                        'size': os.path.getsize(image_path),\n",
    "                        'regions': regions,\n",
    "                        'file_attributes': {\n",
    "                            'plant': class_map[labels[0].item()]['plant'] if len(labels) > 0 else 'unknown'\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                annotation_path = os.path.join(pseudo_annotations_dir, f\"{base_name}_pseudo.json\")\n",
    "\n",
    "                with open(annotation_path, 'w') as f:\n",
    "                    json.dump(pseudo_annotation, f, indent=2)\n",
    "\n",
    "                mask_dir = os.path.join(pseudo_masks_dir, class_map[labels[0].item()]['plant'])\n",
    "                os.makedirs(mask_dir, exist_ok=True)\n",
    "                \n",
    "                Image.fromarray(semantic_mask).save(\n",
    "                    os.path.join(mask_dir, f\"{base_name}_semantic.png\")\n",
    "                )\n",
    "                Image.fromarray(instance_mask).save(\n",
    "                    os.path.join(mask_dir, f\"{base_name}_instance.png\")\n",
    "                )\n",
    "\n",
    "                processed_count += 1\n",
    "                print(f\"Generated pseudo labels for {filename}: {len(regions)} instances\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"\\nPseudo Label Generation Summary:\")\n",
    "    print(f\"Processed: {processed_count} images\")\n",
    "    print(f\"Total pseudo instances: {total_instances}\")\n",
    "    print(f\"Average instances per image: {total_instances/processed_count:.1f}\" if processed_count > 0 else 0)\n",
    "\n",
    "    return processed_count, total_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a2a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model_path, test_image_path, save_path=None):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = get_instance_model(28)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(test_image_path).convert('RGB')\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((1200,1200)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    with torch.no_grad():\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        predictions = model(image_tensor)\n",
    "\n",
    "    pred = predictions[0]\n",
    "    confidence_threshold = 0.5\n",
    "\n",
    "    keep = pred['scores'] > confidence_threshold\n",
    "    boxes = pred['boxes'][keep].cpu().numpy()\n",
    "    labels = pred['labels'][keep].cpu().numpy()\n",
    "    masks = pred['masks'][keep].cpu().numpy()\n",
    "    scores = pred['scores'][keep].cpu().numpy()\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    image_with_boxes = np.array(image)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x1, y1, x2, y2 = box\n",
    "        cv2.rectangle(image_with_boxes, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
    "        cv2.putText(image_with_boxes, f'{label}: {score:.2f}', \n",
    "                   (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        \n",
    "    axes[1].imshow(image_with_boxes)\n",
    "    axes[1].set_title(f'Predictions (conf > {confidence_threshold})')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    combined_mask = np.zeros((1200, 1200, 3))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(masks)))\n",
    "\n",
    "    for i, (mask, color) in enumerate(zip(masks, colors)):\n",
    "        mask_resized = cv2.resize(mask[0], (1200, 1200))\n",
    "        mask_binary = mask_resized > 0.5\n",
    "        combined_mask[mask_binary] = color[:3]\n",
    "\n",
    "    axes[2].imshow(combined_mask)\n",
    "    axes[2].set_title('Instance Masks')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Detected {len(boxes)} instances\")\n",
    "    for i, (label, score) in enumerate(zip(labels, scores)):\n",
    "        print(f\"Instance {i+1}: Class {label}, Confidence {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b4a41f",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f311686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FRUIT INSTANCE SEGMENTATION TRAINING PIPELINE ===\n",
      "\n",
      "1. Training Instance Segmentation Model...\n",
      "Training on device: cpu\n",
      "Loaded 421 images with annotations\n",
      "Training set: 336 images\n",
      "Validation set: 85 images\n",
      "\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lokna\\Projects\\SmartHarvest\\Model_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FRUIT INSTANCE SEGMENTATION TRAINING PIPELINE ===\")\n",
    "print(\"\\n1. Training Instance Segmentation Model...\")\n",
    "\n",
    "trained_model = train_instance_model()\n",
    "\n",
    "# print(\"\\n2. Generating Pseudo Labels...\")\n",
    "# model_path = \"instance_checkpoints/SmartHarvest.pth\"\n",
    "# unlabeled_dir = \"unlabeled_images/\" \n",
    "# pseudo_output_dir = \"pseudo_labels/\"\n",
    "# \n",
    "# if os.path.exists(model_path) and os.path.exists(unlabeled_dir):\n",
    "#     processed, total = generate_pseudo_labels_with_maskrcnn(\n",
    "#         model_path, unlabeled_dir, pseudo_output_dir, \n",
    "#         confidence_threshold=0.7\n",
    "#     )\n",
    "#     print(f\"Generated pseudo labels for {processed} images with {total} instances\")\n",
    "# else:\n",
    "#     print(\"Skipping pseudo label generation - model or unlabeled images not found\")\n",
    "\n",
    "print(\"\\n3. Test Prediction...\")\n",
    "\n",
    "test_image = \"Pictures/Cherry/CA-cherries-split2.jpg\"\n",
    "if os.path.exists(\"instance_checkpoints/final_model.pth\") and os.path.exists(test_image):\n",
    "    visualize_predictions(\"instance_checkpoints/final_model.pth\", test_image, \"prediction_visualization.png\")\n",
    "else:\n",
    "    print(\"Skipping visualization - model or test image not found\")\n",
    "    \n",
    "print(\"\\n=== PIPELINE COMPLETE ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
